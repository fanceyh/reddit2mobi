<html>
<head><title>AMA: Michael I Jordan</title></head>
<body>
<p class=title><!-- SC_OFF --><div class="md"><p>Michael I. Jordan is the Pehong Chen Distinguished Professor in the Department of Electrical Engineering and Computer Science and the Department of Statistics at the University of California, Berkeley. He received his Masters in Mathematics from Arizona State University, and earned his PhD in Cognitive Science in 1985 from the University of California, San Diego. He was a professor at MIT from 1988 to 1998. His research interests bridge the computational, statistical, cognitive and biological sciences, and have focused in recent years on Bayesian nonparametric analysis, probabilistic graphical models, spectral methods, kernel machines and applications to problems in distributed computing systems, natural language processing, signal processing and statistical genetics. Prof. Jordan is a member of the National Academy of Sciences, a member of the National Academy of Engineering and a member of the American Academy of Arts and Sciences. He is a Fellow of the American Association for the Advancement of Science. He has been named a Neyman Lecturer and a Medallion Lecturer by the Institute of Mathematical Statistics. He received the David E. Rumelhart Prize in 2015 and the ACM/AAAI Allen Newell Award in 2009. He is a Fellow of the AAAI, ACM, ASA, CSS, IEEE, IMS, ISBA and SIAM. </p>
</div><!-- SC_ON --></p>
<ul>
<li><div class="md"><p>There has been a ML reading list of books in hacker news for a while, where you recommend some books to start on ML. (<a href="https://news.ycombinator.com/item?id=1055042">https://news.ycombinator.com/item?id=1055042</a>)</p>

<p>Do you still think this is the best set of books, and would you add any new ones?</p>
</div>
<ul>
<li>Author: <div class="md"><p>That list was aimed at entering PhD students at Berkeley,<br/>
who I assume are going to devote many decades of their lives to
the field, and who want to get to the research frontier fairly
quickly.  I would have prepared a rather different list if the
target population was (say) someone in industry who needs enough
basics so that they can get something working in a few months.</p>

<p>That particular version of the list seems to be one from a few
years ago; I now tend to add some books that dig still further
into foundational topics.  In particular, I recommend A. Tsybakov&#39;s
book &quot;Introduction to Nonparametric Estimation&quot; as a very readable
source for the tools for obtaining lower bounds on estimators, and
Y. Nesterov&#39;s very readable &quot;Introductory Lectures on Convex Optimization&quot; as a way to start to understand lower bounds in optimization.  I also recommend A. van der Vaart&#39;s &quot;Asymptotic Statistics&quot;, a book that we often teach from at Berkeley, as a book that shows how many ideas in inference (M estimation---which includes maximum likelihood and empirical risk minimization---the bootstrap, semiparametrics, etc) repose on top of empirical process theory.  I&#39;d also include B. Efron&#39;s &quot;Large-Scale Inference: Empirical Bayes Methods for Estimation, Testing, and Prediction&quot;, as a thought-provoking book.  </p>

<p>I don&#39;t expect anyone to come to Berkeley having read any of these books in entirety, but I do hope that they&#39;ve done some sampling and spent some quality time with at least some parts of most of them.  Moreover, not only do I think that you should eventually read all of these books (or some similar list that reflects your own view of foundations), but I think that you should read all of them three times---the first time you barely understand, the second time you start to get it, and the third time it all seems obvious.</p>

<p>I&#39;m in it for the long run---three decades so far, and hopefully a few more. I think that that&#39;s true of my students as well.  Hence the focus on foundational ideas.</p>
</div>
<ul>
<li><div class="md"><p>Amazing, thanks for the answer, I&#39;ve gone through many of the books at least once, I was a PhD student, now a postdoc doing mostly applied ML.</p>

<p>That is the reason I&#39;ve used that list as my reference list.</p>

<p>Just as a side comment:</p>

<p>Tsybakov&#39;s book is available online at Springer if your University has access to it: <a href="http://link.springer.com/book/10.1007%2Fb13794">http://link.springer.com/book/10.1007%2Fb13794</a></p>

<p>Nesterov&#39;s books is also available: <a href="http://link.springer.com/book/10.1007%2F978-1-4419-8853-9">http://link.springer.com/book/10.1007%2F978-1-4419-8853-9</a></p>

<p>Again, thanks for the answer</p>
</div>
<ul>
<li><div class="md"><blockquote>
<p>Nesterov&#39;s books is also available: <a href="http://link.springer.com/book/10.1007%2F978-1-4419-8853-9">http://link.springer.com/book/10.1007%2F978-1-4419-8853-9</a></p>
</blockquote>

<p>Nice! I&#39;ve been looking for a book like this, thanks.</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>Will you prepare a list for someone in industry? Please.</p>
</div>
<ul>
<li><div class="md"><p>[deleted]</p>
</div>
<ul>
<li><div class="md"><p>&#39;For hackers&#39; -&gt; &#39;I don&#39;t want to learn any math&#39;</p>
</div>
</li>
<li><div class="md"><p>Good advice. Thanks</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>In this post, I would like to blend together recommendations from academic and industry researchers: <a href="http://nzhiltsov.blogspot.com/2014/09/highly-recommended-books-for-machine-learning-researchers.html">http://nzhiltsov.blogspot.com/2014/09/highly-recommended-books-for-machine-learning-researchers.html</a></p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>Thanks a lot! BTW, I gathered your recommendations on Goodreads:  <a href="https://www.goodreads.com/review/list/6324945-nikita-zhiltsov?shelf=m-jordan-s-list">https://www.goodreads.com/review/list/6324945-nikita-zhiltsov?shelf=m-jordan-s-list</a></p>
</div>
<ul>
<li><div class="md"><p>Other comprehensive list of video lectures and some books. 
<a href="http://dk-techlogic.blogspot.in/2012/05/best-machine-learning-resources.html">http://dk-techlogic.blogspot.in/2012/05/best-machine-learning-resources.html</a></p>
</div>
</li>
<li><div class="md"><p>Nice work.</p>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><div class="md"><p>ain&#39;t nobody got time for all that</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>Do you expect more custom, problem specific graphical models to outperform the ubiquitous, deep, layered, boringly similar neural networks in the future?</p>
</div>
<ul>
<li>Author: <div class="md"><p>OK, I guess that I have to say something about &quot;deep learning&quot;.
This seems like as good a place as any (apologies, though, for
not responding directly to your question).  </p>

<p>&quot;Deep&quot; breath.</p>

<p>My first and main reaction is that I&#39;m totally happy that any area
of machine learning (aka, statistical inference and decision-making;
see my other post :-) is beginning to make impact on real-world
problems.  I&#39;m in particular happy that the work of my long-time
friend Yann LeCun is being recognized, promoted and built upon.
Convolutional neural networks are just a plain good idea.</p>

<p>I&#39;m also overall happy with the rebranding associated with the usage
of the term &quot;deep learning&quot; instead of &quot;neural networks&quot;.  In other
engineering areas, the idea of using pipelines, flow diagrams and
layered architectures to build complex systems is quite well entrenched,
and our field should be working (inter alia) on principles for building
such systems.  The word &quot;deep&quot; just means that to me---layering (and I
hope that the language eventually evolves toward such drier words...).
I hope and expect to see more people developing architectures that
use other kinds of modules and pipelines, not restricting themselves
to layers of &quot;neurons&quot;.</p>

<p>With all due respect to neuroscience, one of the major scientific areas
for the next several hundred years, I don&#39;t think that we&#39;re at the point
where we understand very much at all about how thought arises in networks of neurons, and I still don&#39;t see neuroscience as a major generator for ideas on how to build inference and decision-making systems in detail. Notions like &quot;parallel is good&quot; and &quot;layering is good&quot; could well (and have) been developed entirely independently of thinking about brains.</p>

<p>I might add that I was a PhD student in the early days of neural networks,
before backpropagation had been (re)-invented, where the focus was on the
Hebb rule and other &quot;neurally plausible&quot; algorithms.  Anything that the
brain couldn&#39;t do was to be avoided; we needed to be pure in order to find
our way to new styles of thinking.  And then Dave Rumelhart started exploring
backpropagation---clearly leaving behind the neurally-plausible constraint---and
suddenly the systems became much more powerful.  This made an impact on me.
Let&#39;s not impose artificial constraints based on cartoon models of topics
in science that we don&#39;t yet understand.</p>

<p>My understanding is that many if not most of the &quot;deep learning success
stories&quot; involve supervised learning (i.e., backpropagation) and massive
amounts of data.  Layered architectures involving lots of linearity,
some smooth nonlinearities, and stochastic gradient descent seem to be
able to memorize huge numbers of patterns while interpolating smoothly
(not oscillating) &quot;between&quot; the patterns; moreover, there seems to be
an ability to discard irrelevant details, particularly if aided by weight-
sharing in domains like vision where it&#39;s appropriate.  There&#39;s also some
of the advantages of ensembling.  Overall an appealing mix.  But this
mix doesn&#39;t feel singularly &quot;neural&quot; (particularly the need for large
amounts of labeled data).</p>

<p>Indeed, it&#39;s unsupervised learning that has always been viewed as the Holy
Grail; it&#39;s presumably what the brain excels at and what&#39;s really going
to be needed to build real &quot;brain-inspired computers&quot;.  But here I have
some trouble distinguishing the real progress from the hype.  It&#39;s my
understanding that in vision at least, the unsupervised learning ideas
are not responsible for some of the recent results; it&#39;s the supervised
training based on large data sets.</p>

<p>One way to approach unsupervised learning is to write down various formal
characterizations of what good &quot;features&quot; or &quot;representations&quot; should look
like and tie them to various assumptions that seem to be of real-world relevance.
This has long been done in the neural network literature (but also far
beyond).  I&#39;ve seen yet more work in this vein in the deep learning work
and I think that that&#39;s great.  But I personally think that the way to go
is to put those formal characterizations into optimization functionals or
Bayesian priors, and then develop procedures that explicitly try to optimize
(or integrate) with respect to them.  This will be hard and it&#39;s an ongoing
problem to approximate.  In some of the deep learning learning work that
I&#39;ve seen recently, there&#39;s a different tack---one uses one&#39;s favorite
neural network architecture, analyses some data and says &quot;Look, it embodies
those desired characterizations without having them built in&quot;.  That&#39;s the
old-style neural network reasoning, where it was assumed that just because
it was &quot;neural&quot; it embodied some kind of special sauce.  That logic didn&#39;t
work for me then, nor does it work for me now.</p>

<p>Lastly, and on a less philosophical level, while I do think of neural networks
as one important tool in the toolbox, I find myself surprisingly rarely going
to that tool when I&#39;m consulting out in industry.  I find that industry people
are often looking to solve a range of other problems, often not involving
&quot;pattern recognition&quot; problems of the kind I associate with neural networks.
E.g., (1) How can I build and serve models within a certain time budget so that I
get answers with a desired level of accuracy, no matter how much data I have?
(2) How can I get meaningful error bars or other measures of performance on
all of the queries to my database?  (3) How do I merge statistical thinking
with database thinking (e.g., joins) so that I can clean data effectively
and merge heterogeneous data sources?  (4) How do I visualize data, and in
general how do I reduce my data and present my inferences so that humans can
understand what&#39;s going on?  (5) How can I do diagnostics so that I don&#39;t
roll out a system that&#39;s flawed or so that I can figure out that an existing system is
now broken?  (6) How do I deal with non-stationarity?  (7) How do I do some
targeted experiments, merged with my huge existing datasets, so that I can
assert that some variables have a causal effect?</p>

<p>Although I could possibly investigate such issues in the context of deep
learning ideas, I generally find it a whole lot more transparent to investigate
them in the context of simpler building blocks.</p>

<p>Based on seeing the kinds of questions I&#39;ve discussed above arising again and
again over the years I&#39;ve concluded that statistics/ML needs a deeper engagement
with people in CS systems and databases, not just with AI people, which has
been the main kind of engagement going on in previous decades (and still remains
the focus of &quot;deep learning&quot;).  I&#39;ve personally been doing exactly that at
Berkeley, in the context of the &quot;RAD Lab&quot; from 2006 to 2011 and in the current
context of the &quot;AMP Lab&quot;.</p>
</div>
<ul>
<li><div class="md"><p>&quot;One way to approach unsupervised learning is to write down various formal characterizations of what good &quot;features&quot; or &quot;representations&quot; should look like and tie them to various assumptions that seem to be of real-world relevance ... one uses one&#39;s favorite neural network architecture, analyses some data and says &#39;Look, it embodies those desired characterizations without having them built in&#39;.&quot;</p>

<p>What if we instead measure the quality of the features by how well they allow our system to interact with the world and solve meaningful problems?  For example, is the model able to learn features that allow us to classify images?  Is it able to learn features that enable effective reinforcement learning?  How well is it able to forecast events in the future?  </p>

<p>For example, if I build a model where the input is a text description of a video and the first half of the video and the task is to model the joint distribution over the pixels in the second half of the video, then success in this task should indicate that the model has learned a meaningful higher level representation of the text, even though we don&#39;t necessarily have a formal notion for what that representation should look like.  </p>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><div class="md"><p>Why do you believe nonparametric models haven&#39;t taken off as well as other work you and others have done in graphical models?</p>
</div>
<ul>
<li>Author: <div class="md"><p>I think that mainly they simply haven&#39;t been tried.  Note that latent Dirichlet allocation is a parametric Bayesian model in which the number of topics K is assumed known.  The nonparametric version of LDA is called the HDP (hierarchical Dirichlet process), and in some very practical sense it&#39;s just a small step from LDA to the HDP (in particular, just a few more lines of code are needed to implement the HDP).  Now LDA has been used in several thousand applications by now, and it&#39;s my strong suspicion that the users of LDA in those applications would have been just as happy using the HDP, if not happier.</p>

<p>One thing that the field of Bayesian nonparametrics really needs is an accessible introduction that presents the math but keeps it gentle---such an introduction doesn&#39;t currently exist.  My colleague Yee Whye Teh and I are nearly done with writing just such an introduction; we hope to be able to distribute it this fall.</p>

<p>I do think that Bayesian nonparametrics has just as bright a future in statistics/ML as classical nonparametrics has had and continues to have.  Models that are able to continue to grow in complexity as data accrue seem very natural for our age, and if those models are well controlled so that they concentrate on parametric sub-models if those are adequate, what&#39;s not to like?</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>I had the great fortune of attending your course on Bayesian Nonparametrics in Como this summer, which was a very educational introduction to the subject, so thank you. I have a few questions on ML theory, nonparametrics, and the future of ML.</p>

<ol>
<li><p>At the course, you spend a good deal of time on the subject of Completely Random Measures and the advantages of employing them in modelling. 
<strong>Do you think there are any other (specific) abstract mathematical concepts or methodologies we would benefit from studying and integrating into ML research?</strong> (another example of an ML field which benefited from such inter-discipline crossover would be Hybrid MCMC, which is grounded in dynamical systems theory)</p></li>
<li><p>It seems that most applications of Bayesian nonparametrics (GPs aside) currently fall into clustering/mixture models, topic modelling, and graph modelling. <strong>What is the next frontier for applied nonparametrics?</strong></p></li>
<li><p>Sometimes I am a bit disillusioned by the current trend in ML of just throwing universal models and lots of computing force at every problem.  <strong>Will this trend continue, or do you think there is hope for less data-hungry methods such as coresets, matrix sketching, random projections, and active learning?</strong></p></li>
</ol>

<p>Thank you for taking the time out to do this AMA.</p>
</div>
<ul>
<li>Author: <div class="md"><p>Great questions, particularly #1.  Indeed I&#39;ve spent much of my career trying out existing ideas from various mathematical fields in new contexts and I continue to find that to be a very fruitful endeavor.  That said, I&#39;ve had way more failures than successes, and I hesitate to make concrete suggestions here because they&#39;re more likely to be fool&#39;s gold than the real thing.</p>

<p>Let me just say that I do think that completely random measures (CRMs) continue to be worthy
of much further attention.  They&#39;ve mainly been used in the context of deriving <em>normalized</em>
random measures (by, e.g., James, Lijoi and Pruenster); i.e., random probability measures.  </p>

<p>Liberating oneself from that normalizing constant is a worthy thing to consider,
and general CRMs do just that.  Also, note that the adjective &quot;completely&quot; refers 
to a useful independence property, one that suggests yet-to-be-invented divide-and-conquer algorithms.  </p>

<p>Basically, I think that CRMs are to nonparametrics what exponential families are to parametrics (and I might note that I&#39;m currently working on a paper with Tamara Broderick and Ashia Wilson that tries to bring that idea to life).  Note also that exponential families seemed to have been dead after Larry Brown&#39;s seminal monograph several decades ago, but they&#39;ve continued to have multiple after-lives (see, e.g., my monograph with Martin Wainwright, where studying the conjugate duality of exponential families led to new vistas).</p>

<p>As for the next frontier for applied nonparametrics, I think that it&#39;s mainly &quot;get
real about real-world applications&quot;.  I think that too few people have tried out Bayesian nonparametrics on real-world, large-scale problems (good counter-examples include Emily Fox at UW and David Dunson at Duke).  Once more courage for real deployment begins to emerge I believe that the field will start to take off.</p>

<p>Lastly, I&#39;m certainly a fan of coresets, matrix sketching, and random projections. 
I view them as basic components that will continue to grow in value as people start to build more complex, pipeline-oriented architectures.  I&#39;m not sure that I&#39;d view them as &quot;less data-hungry methods&quot;, though; essentially they provide a scalability knob that allows systems to take in more data while 
still retaining control over time and accuracy.</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>In industries dealing with uncertainty, there seems to be a persistent divide between those who need effective results, and those who need more theoretically guaranteed results (classical significance testing comes to mind). This often seems to parallel the backgrounds of workers in the former in machine learning and the latter in statistics. How do you envision the two communities&#39; joint futures evolving?</p>

<p>Do you think methods more squarely in the realm of machine learning will be used by scientists in the future to &quot;prove&quot; results the way statistics is used today?</p>

<p>Is there anything particular you think one community would do well to recognize in the other (particular methods, algorithms, central questions, academic culture, whatever)?</p>

<p>Is there anything you think is important to one side and not the other?</p>
</div>
<ul>
<li>Author: <div class="md"><p>I personally don&#39;t make the distinction between statistics
and machine learning that your question seems predicated on.</p>

<p>Also I rarely find it useful to distinguish between theory
and practice; their interplay is already profound and will
only increase as the systems and problems we consider grow
more complex.</p>

<p>Think of the engineering problem of building a bridge.  There&#39;s
a whole food chain of ideas from physics through civil engineering
that allow one to design bridges, build them, give guarantees that
they won&#39;t fall down under certain conditions, tune them to specific
settings, etc, etc.  I suspect that there are few people involved
in this chain who don&#39;t make use of &quot;theoretical concepts&quot; and
&quot;engineering know-how&quot;.  It took decades (centuries really) for
all of this to develop.</p>

<p>Similarly, Maxwell&#39;s equations provide the theory behind electrical
engineering, but ideas like impedance matching came into focus as
engineers started to learn how to build pipelines and circuits.
Those ideas are both theoretical and practical.</p>

<p>We have a similar challenge---how do we take core inferential ideas
and turn them into engineering systems that can work under whatever
requirements that one has in mind (time, accuracy, cost, etc),
that reflect assumptions that are appropriate for the domain, that
are clear on what inferences and what decisions are to be made
(does one want causes, predictions, variable selection, model selection,
ranking, A/B tests, etc, etc), can allow interactions with humans
(input of expert knowledge, visualization, personalization, privacy,
ethical issues, etc, etc), that scale, that are easy to use and are
robust.  Indeed, with all due respect to bridge builders (and rocket
builders, etc), but I think that we have a domain here that is more
complex than any ever confronted in human society.</p>

<p>I don&#39;t know what to call the overall field that I have in mind here
(it&#39;s fine to use &quot;data science&quot; as a placeholder), but the main point
is that most people who I know who were trained in statistics or in
machine learning implicitly understood themselves as working in this
overall field; they don&#39;t say &quot;I&#39;m not interested in principles having
to do with randomization in data collection, or with how to merge data,
or with uncertainty in my predictions, or with evaluating models, or
with visualization&quot;.  Yes, they work on subsets of the overall problem,
but they&#39;re certainly aware of the overall problem.  Different collections
of people (your &quot;communities&quot;) often tend to have different application
domains in mind and that makes some of the details of their current work
look superficially different, but there&#39;s no actual underlying intellectual
distinction, and many of the seeming distinctions are historical accidents.</p>

<p>I also must take issue with your phrase &quot;methods more squarely in the
realm of machine learning&quot;.  I have no idea what this means, or could
possibly mean.  Throughout the eighties and nineties, it was striking
how many times people working within the &quot;ML community&quot; realized that
their ideas had had a lengthy pre-history in statistics.  Decision trees,
nearest neighbor, logistic regression, kernels, PCA, canonical correlation,
graphical models, K means and discriminant analysis come to mind, and also
many general methodological principles (e.g., method of moments, which
is having a mini-renaissance, Bayesian inference methods of all kinds,
M estimation, bootstrap, cross-validation, EM, ROC, and of course stochastic
gradient descent, whose pre-history goes back to the 50s and beyond),
and many many theoretical tools (large deviations, concentrations, empirical
processes, Bernstein-von Mises, U statistics, etc).  Of course, the &quot;statistics
community&quot; was also not ever that well defined, and while ideas such as
Kalman filters, HMMs and factor analysis originated outside of the &quot;statistics
community&quot; narrowly defined, there were absorbed within statistics because
they&#39;re clearly about inference.  Similarly, layered neural networks can and
should be viewed as nonparametric function estimators, objects to be analyzed
statistically.</p>

<p>In general, &quot;statistics&quot; refers in part to an analysis style---a statistician
is happy to analyze the performance of any system, e.g., a logic-based
system, if it takes in data that can be considered random and outputs decisions
that can be considered uncertain.  A &quot;statistical method&quot; doesn&#39;t have to have
any probabilities in it per se.  (Consider computing the median).</p>

<p>When Leo Breiman developed random forests, was he being a statistician or
a machine learner?  When my colleagues and I developed latent Dirichlet
allocation, were we being statisticians or machine learners?  Are the SVM
and boosting machine learning while logistic regression is statistics, even
though they&#39;re solving essentially the same optimization problems up to
slightly different shapes in a loss function?  Why does anyone think that
these are meaningful distinctions?</p>

<p>I don&#39;t think that the &quot;ML community&quot; has developed many new inferential
principles---or many new optimization principles---but I do think that the
community has been exceedingly creative at taking existing ideas across
many fields, and mixing and matching them to solve problems in emerging
problem domains, and I think that the community has excelled at making
creative use of new computing architectures.  I would view all of this as the
proto emergence of an engineering counterpart to the more purely theoretical
investigations that have classically taken place within statistics and optimization.  </p>

<p>But one shouldn&#39;t definitely not equate statistics or optimization with theory
and machine learning with applications.  The &quot;statistics community&quot; has also been
very applied, it&#39;s just that for historical reasons their collaborations
have tended to focus on science, medicine and policy rather than engineering.
The emergence of the &quot;ML community&quot; has (inter alia) helped to enlargen the
scope of &quot;applied statistical inference&quot;.  It has begun to break down some barriers
between engineering thinking (e.g., computer systems thinking) and inferential
thinking.  And of course it has engendered new theoretical questions.</p>

<p>I could go on (and on), but I&#39;ll stop there for now...</p>
</div>
<ul>
<li><div class="md"><p>That&#39;s exactly the kind of thing I was hoping to pick your brain over. Thanks for the extremely thoughtful answer, and thanks for stopping by today.</p>
</div>
</li>
<li><div class="md"><p>If only more people understood this</p>
</div>
<ul>
<li><div class="md"><p>People don&#39;t understand this because they try to apply algorithms without understanding inference. From what I&#39;ve seen (both in online forums and at work), 95% of fancy &quot;machine learning&quot; algorithms are thrown at data by someone who has only the most superficial understanding of what they&#39;re actually doing.</p>
</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><div class="md"><p>What the future holds for probabilistic graphical models? Anything beyond CRFs?</p>
</div>
<ul>
<li>Author: <div class="md"><p>Probabilistic graphical models (PGMs) are one way to express structural aspects of joint probability distributions, specifically in terms of conditional independence relationships and other factorizations.  That&#39;s a useful way to capture some kinds of structure, but there are lots of other structural aspects of joint probability distributions that one might want to capture, and PGMs are not necessarily going to be helpful in general.  There is not ever going to be one general tool that is dominant; each tool has its domain in which its appropriate.  Think literally of a toolbox.  We have hammers, screwdrivers, wrenches, etc, and big projects involve using each of them in appropriate (although often creative) ways.</p>

<p>On the other hand, despite having limitations (a good thing!), there is still lots to explore in PGM land.  Note that many of the most widely-used graphical models are chains---the HMM is an example, as is the CRF.  But beyond chains there are trees and there is still much to do with trees.  Note that latent Dirichlet allocation is a tree.  (And in 2003 when we introduced LDA, I can remember people in the UAI community who had been-there-and-done-that for years with trees saying: &quot;but it&#39;s just a tree; how can that be worthy of more study?&quot;).  And I continue to find much inspiration in tree-based architectures, particularly for problems in three big areas where trees arise organically---evolutionary biology, document modeling and natural language processing.  For example,
I&#39;ve worked recently with Alex Bouchard-Cote on evolutionary trees, where the entities propagating along the edges of the tree are strings of
varying length (due to deletions and insertions), and one wants to
infer the tree and the strings.  In the topic modeling domain, I&#39;ve
been very interested in multi-resolution topic trees, which to me are
one of the most promising ways to move beyond latent Dirichlet allocation. John Paisley, Chong Wang, Dave Blei and I have developed something called the nested HDP in which documents aren&#39;t just vectors but they&#39;re multi-paths down trees of vectors.  Lastly, Percy Liang, Dan Klein and I have worked on a major project in natural-language semantics, where the basic model is a tree (allowing syntax and semantics to interact easily), but where nodes can be set-valued, such that the classical constraint satisfaction (aka, sum-product) can handle some of the &quot;first-order&quot; aspects of semantics.</p>

<p>This last point is worth elaborating---there&#39;s no reason that one can&#39;t
allow the nodes in graphical models to represent random sets, or random
combinatorial general structures, or general stochastic processes; factorizations can be just as useful in such settings as they are in the classical settings of random vectors.  There&#39;s still lots to explore there.</p>
</div>
<ul>
<li><div class="md"><p>Thank you for your answer, prof. Jordan!</p>

<p>In the context of natural language processing, what paper would you recommend to understand the applicability of trees?</p>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><div class="md"><p>If you got a billion dollars to spend on a huge research project that you get to lead, what would you like to do?</p>
</div>
<ul>
<li>Author: <div class="md"><p>Having just written (see above) about the need for statistics/ML to ally itself more with CS systems and database researchers rather than focusing mostly on AI, let me take the opportunity of your question to exhibit my personal incoherence and give an answer that focuses on AI.</p>

<p>I&#39;d use the billion dollars to build a NASA-size program focusing on natural language processing (NLP), in all of its glory (semantics, pragmatics, etc).</p>

<p>Intellectually I think that NLP is fascinating, allowing us to focus on highly-structured inference problems, on issues that go to the core of &quot;what is thought&quot; but remain eminently practical, and on a technology that surely would make the world a better place.</p>

<p>Although current deep learning research tends to claim to encompass NLP, I&#39;m (1) much less convinced about the strength of the results, compared to the results in, say, vision; (2) much less convinced in the case of NLP than, say, vision, the way to go is to couple huge amounts of data with black-box learning architectures.</p>

<p>I&#39;d invest in some of the human-intensive labeling processes that one sees in projects like FrameNet and (gasp) projects like Cyc.  I&#39;d do so in the context of a full merger of &quot;data&quot; and &quot;knowledge&quot;, where the representations used by the humans can be connected to data and the representations used by the learning systems are directly tied to linguistic structure.  I&#39;d do so in the context of clear concern with the usage of language (e.g., causal reasoning).</p>

<p>Very challenging problems, but a billion is a lot of money.  (Isn&#39;t it?).</p>
</div>
<ul>
<li><div class="md"><p>Isn&#39;t google doing some work in natural language processing with Ray Kurzweil?</p>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><div class="md"><p>What are the most important high level trends in machine learning research and industry applications these days?</p>
</div>
<ul>
<li>Author: <div class="md"><p>See the numbered list at the end of my blurb on deep learning above.  These are a few examples of what I think is the major meta-trend, which is the merger of statistical thinking and computational thinking.</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>Over the past 3 years we&#39;ve seen some notable advancements in efficient approximate posterior inference for topic models and Bayesian nonparametrics e.g. Hoffman 2011, Chong Wang 2011, Tamara Broderick&#39;s and your 2013 NIPS work, your recent work with Paisley, Blei and Wang on extending stochastic inference to the nested Hierarchical Dirichlet Process.</p>

<p>One characteristic of your &quot;extended family&quot; of researchers has always been a knack for implementing complex models using real-world, non-trivial data sets such as Wikipedia or the New York Times archive. </p>

<p>In that spirit of implementing, which topic modeling application areas are you most excited about at the moment and looking forward, what impact do you think these recent developments in fast, scalable inference for conjugate and conditionally conjugate Bayes nets will have on the applications we develop 5-10 years from now?</p>
</div>
</li>
<li><div class="md"><p>Do you mind explaining the history behind how you learned about variational inference as a graduate student? What current techniques do you think students should be learning now to prepare for future advancements in approximate inference? </p>
</div>
</li>
<li><div class="md"><p>How is your typical day structured?</p>
</div>
<ul>
<li>Author: <div class="md"><p>I spend half of each day minimizing entropy and half of each day maximizing entropy.</p>

<p>(The exact mix isn&#39;t one half, and indeed it&#39;s the precise calibration of that number which is the main determiner of my happiness in life.)</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>What are your thoughts on the recent work on Deep Generative Models and Stochastic Backpropagation [refs: <a href="http://arxiv.org/abs/1401.4082">1</a>, <a href="http://jmlr.org/proceedings/papers/v32/bengio14.pdf">2</a>, <a href="http://www.ics.uci.edu/%7Ewelling/publications/papers/NN2BN_ICML14.pdf">3</a>]?  </p>
</div>
</li>
<li><div class="md"><p>Dear Dr. Jordan would you mind providing us with some useful information regarding students embarking on their journey to PhD.</p>

<ul>
<li>For instance what are some common pitfalls that PhD students should avoid?</li>
<li>In contrast, what are some good practices that PhD students should strive for?</li>
<li>What values are you looking for in prospective PhD students that join your lab?</li>
<li>During my experience I have realized that most researchers are looking only for the super stars to join their labs leaving the rest vanishing in the thin air. What is the meaning of accepting someone that is already highly regarded in the scientific community (apart from the probability that s/he will be successfull in the program and will boost the image of the lab)? Isn&#39;t the whole point of PhD to transform people in the process even those who might not have the same credentials as the super stars?</li>
<li>A side effect of the above process will lead to marginalization of people who might really have a strong interest in research. What is your opinion on this subject?</li>
</ul>

<p>Thank you!</p>
</div>
</li>
<li><div class="md"><p>When neural networks are used to model a probability distribution, it is common to not make any hard independence assumptions (i.e. assume that the graphical model is fully connected).  While this makes the model more general and more likely to be accurate for large datasets, it makes learning intractable for very large datasets (for example, getting the joint distribution over millions of random variables).  </p>

<p>What areas of research do you see leading to improvement in large scale probabilistic modeling in cases where it is difficult to make explicit independence assumptions?  </p>
</div>
</li>
<li><div class="md"><p>Dear Dr. Jordan, </p>

<p>1) In your talk &quot;Statistical Inference of Protein Structures&quot; on videolectures.net, you seemed a bit surprised that the field of Structural Biology didn&#39;t know to do regularized logistic regression for catalytic site detection, and you were able to outdo the state of the art using fairly simple methods. By your estimation, what could be done to reduce the lag between statistics/ML and the &#39;applied&#39; side of things? (As someone who is a bioinformatics person and has read several papers on nonparametric Bayesian methods without any implementational know-how to show for it, I&#39;d like to prime this answer with &quot;readable code examples&quot; ;)) </p>

<p>2) What is your opinion on the burgeoning field of representation learning? There seems to be a lot of buzz in the NLP community about representing atomic symbols with high-dimensional vectors that are adjusted by backpropagation to improve prediction. This mirrors a trend in Cognitive Science where certain systems have shown to be capable of analogical reasoning using high-dimensional (nearly orthogonal) random vectors to represent atomic concepts, as their combinations yield so-called graded representations (vectors that are similar to their constituents and nearly orthogonal to anything else). You are fairly invested in the Bayesian side of things - is this just a conceptual distraction egged on by the allure of &quot;neurally plausible&quot; systems, or might they be on to something? </p>

<p>Thank you so much for taking the time! </p>
</div>
</li>
<li><div class="md"><p>What do you think of probabilistic programming? </p>
</div>
</li>
<li><div class="md"><p>Do you have any good stories of people expecting a 6&#39;6&quot; athlete?  </p>

<p>sorry I don&#39;t have any important question, always wondered. </p>
</div>
<ul>
<li><div class="md"><p>my question was going to be: &quot;Do you consider yourself to be the Michael Jordan of machine learning?&quot; I will just group it here with your unrelated question to be downvoted into oblivion. </p>
</div>
</li>
<li><div class="md"><p>I remember there was actually once a faculty vs students basketball tournament, and I&#39;m pretty sure Professor Jordan&#39;s name was on the flyer.</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>What are your thoughts on compressive sensing and dictionary learning through sparse representations? How far can greedy approaches and local optimum solutions go?</p>
</div>
</li>
<li><div class="md"><p>What do you think of recurrent networks like spiking neural networks? Do you think they have a chance of surpassing the performance of deep learning once we find better algorithms? Or are they going to stay relevant for the biological domain only?</p>
</div>
</li>
<li><div class="md"><p>Are you planning to release your book on graphical models? If yes then will it be happening soon?</p>
</div>
</li>
<li><div class="md"><p>If you would have to bet, what would you say it is the branch of Machine Learning / Data Science that has the biggest chance of making a big breakthrough?</p>
</div>
</li>
<li><div class="md"><p>Dear Dr. Jordan, you did a lot of research in sensori-motor learning. You proposed that there is an internal model model in the brain which simulates the body in order to plan movements and also in order to learn new movements. Where did this idea originate and how did you get in contact with scientists from sensori-motor learning research? </p>
</div>
</li>
<li><div class="md"><p>A large amount of your former students have very successful academic careers. Do you have any advice for a current grad student hoping to pursue an academic career in statistics/ML/signal processing?</p>

<p>edit: Perhaps I should be a bit more specific, do you have any advice regarding:</p>

<ul>
<li>Keeping up with literature</li>
<li>Choosing research topics</li>
</ul>

<p>Also how do you think that learning mathematics outside of that which has direct applications to research is a good way to spend time? I have been doing quite a bit of meandering self study into functional analysis, but I&#39;m wondering if I should try to keep it relevant to established statistical topics (ie Hilbert space theory for kernel methods and such).</p>
</div>
</li>
<li><div class="md"><p>Should I go back to the university to catch up with rapid advance of machine learning (especially deep learning) technologies?</p>

<p>I&#39;m a research engineer in a commercial company, and I&#39;m mainly working to apply machine learning techniques to our business.
I studied only basics of machine learning in my B.E and M.E course since I was more interested in and focused on its application in business intelligence.
But now I feel strong anxiety that recent intensive researches on deep learning is going to leave me far behind the frontier of technology. 
So I think I should go to a Ph.D course in machine learning to train myself in more academic and theoretical area of the discipline, otherwise I would be just valueless in a few years.</p>

<p>I would be really grad if you gave me any kind of advise.</p>
</div>
</li>
<li><div class="md"><p>I&#39;m wondering what&#39;s your opinion about copulas as a fancy statistical tool to evaluate distributions and describe dependencies between random variables. Why aren&#39;t many their applications in fields other than economics, such as text mining and information retrieval out there?</p>
</div>
<ul>
<li><div class="md"><p>Lots of people are thinking about copulas in ML.</p>

<p>NIPS Workshop on Copulas in ML: <a href="http://pluto.huji.ac.il/%7Egalelidan/CopulaWorkshop/index.html">http://pluto.huji.ac.il/~galelidan/CopulaWorkshop/index.html</a></p>

<p>Something like 30 results on the NIPS website for papers related to copulas: <a href="http://papers.nips.cc/search/?q=copula">http://papers.nips.cc/search/?q=copula</a></p>

<p>Many more JMLR and ICML papers if you do a rudimentary Google search.</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>What do you think the role of online coursework is and will be in the machine learning world?  Do you think the concept of online coursework can be a useful tool to facilitate collaborations between industry and the academic world?</p>
</div>
</li>
<li><div class="md"><p>What are your thoughts on quantum computing? </p>
</div>
</li>
<li><div class="md"><p>Do you have an opinion on the Machine Intelligence Research Institute, and the work that they do?</p>

<p>(Nine days late.  I wonder if you&#39;re still checking the thread...)</p>
</div>
</li>
<li><div class="md"><p>What new machine learning algorithms should I be aware of other than deep learning and svms?</p>

<p>And where do you think the researchers is focusing on right now?</p>
</div>
</li>
<li><div class="md"><p>How well do generative models (e.g. Latent Dirichlet Allocation) compare to some of the more recently used deep architectures (e.g. Sum-Product Networks)?</p>

<p>Do you see any potential for these methods to perform well enough to say make an image, song, or coherent short story?</p>

<p>Thank you!</p>
</div>
</li>
<li><div class="md"><p>Can you talk a little about the qualifications/qualities that you look for in a new post-doc?</p>
</div>
</li>
<li><div class="md"><p>Why the Dirichlet Distribution (DD)? What is so beautiful about this distribution that made you choose it for efficient inference in discrete probability spaces?  Do the geometrical properties of Simplex spaces and, particularly, of neutrality of the DD mean something more fundamental about the way second-order (probabilistic) uncertainty can be represented and handled in artificial intelligence?</p>
</div>
</li>
<li><div class="md"><p>[deleted]</p>
</div>
<ul>
<li><div class="md"><p>I&#39;ll answer this, as a Berkeley grad student: a 3.28 GPA is low for top schools, depending on where it&#39;s from, but research experience is generally much more important than GPA in PhD admissions. If you&#39;ve done original research in machine learning, published in reputable venues, and if you can a) get strong letters of recommendation from your previous research supervisors (ideally these would be academics, but PhD-holding researchers in industry are okay as long as they are still involved with the research community) and b) write a personal statement that tells a compelling story about the research interests you&#39;d like to pursue in grad school and how these have developed since you left undergrad, then you probably have a shot at many strong schools. </p>

<p>Your chances of getting into a specific top program, e.g., Berkeley, are low, because these schools receive tons of applications and <em>everyone&#39;s</em> chances are low (excepting the rare superstars that get in everywhere). But depending on your personal interests, there&#39;s likely a wide range of schools with people doing worthwhile work that you could have a good experience at. People wildly overrate the value of prestigious schools: Berkeley is great, but ultimately the most important thing is finding an adviser you click with and a research direction you&#39;re passionate about; I have friends at lower-ranked schools that have had much more successful graduate careers than some at Berkeley (and vice versa, of course). I&#39;d look at the top 30-40 schools in the USNews CS grad program rankings, filter for those that have faculty in your area of interest you&#39;d be excited to work with, then apply to a broad spattering of schools at different ranks. You may or may not get into a top program, but you have a decent chance of ending up with at least a few good options. (of course, you should ask the people writing your recommendations, who know you much better, for specific advice about the strength of your application and where to apply). </p>
</div>
</li>
<li><div class="md"><p>I don&#39;t know why you have so many downvotes...</p>
</div>
<ul>
<li><div class="md"><p>Professors at top schools get asked this kind of nonsense all of the time. It&#39;s quite well established that it is in bad taste to ask researchers what your chances of getting into their programs are.</p>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><div class="md"><p>What&#39;s your opinion about Random Forest?</p>
</div>
</li>
<li><div class="md"><p>As a current postdoc in ML, what are the areas in Machine Learning that you see having a big impact in the coming years? What should I focus on trying to learn more about?</p>
</div>
</li>
<li><div class="md"><p>Which private companies do you think are applying advanced statistical methods to the most interesting problems or in the most interesting ways?</p>
</div>
</li>
<li><div class="md"><p>Michael Jordan -- the basketball player -- left a very lucrative career in basketball where he was, and still is regarded as a true expert in the field. When he left, he played baseball and was pretty terrible at it.</p>

<p>What career change(s)—or deviation(s)—from your primary expertise have you made that didn&#39;t really pan out? That is: Michael Jordan&#39;s Basketball career is to Michael Jordan&#39;s Baseball career as your career in [a broad domain of modeling] is to your dive into [blank]. </p>

<p>And, why didn&#39;t it pan it? </p>
</div>
</li>
<li><div class="md"><p>I am learning a lot from the tutorials you have posted on your site.  Thanks!  Would you be able to post the lecture slides for the statistical learning theory course that you taught in the Spring? (<a href="http://www.cs.berkeley.edu/%7Ejordan/courses/281A-spring14/">http://www.cs.berkeley.edu/~jordan/courses/281A-spring14/</a>)</p>
</div>
</li>
<li><div class="md"><p>What is your lab environment like?  Is there a lot of collaborations between your students? Any memorable firings?  Any lab romances?  </p>
</div>
</li>
<li><div class="md"><p>Data collection has become quite prevalent, but there are still academic and industrial fields that dislike probability-based inferential methods.  Do you have any advice for communicating ideas to the uninitiated?  </p>
</div>
</li>
<li><div class="md"><p>Thank you for doing the AMA Dr. Jordan! Do you believe in an inevitable Singularity (per Ray Kurzweil)? If so, when do you expect it to reach the masses?</p>
</div>
</li>
<li><div class="md"><p>Can anyone learn everything in Machine Learning? </p>

<p>It seems there is a lot of variation in this small sub-field. How does one try to consume so much over the years and try to understand the field? Given someone is really curious and will dedicate his life for his learning and maybe contribute on the way. I know it can sound a bit selfish to be just learning and not aiming to contribute(possibly curiously might solve few problem) but what is the best shot to go through breadth and depth over the years. </p>

<p>How was your learning experience and exposure timeline through your career?</p>

<p>Thanks!!</p>
</div>
</li>
<li><div class="md"><p>What do you think of the current state of the ML field? Where did we came from and where are we are we going? What problems are you currently excited about? How do you decide what problems are worth pursuing? Are you going to NIPS this year? What makes a well-rounded machine learning researcher? What do you strive for?  How would you describe yourself as a PhD advisor? How is for a PhD student to work in your group? Do you still have a lot of time for advising? How did you manage to train so many top researchers from the field (either as PhDs or Post-Docs: Wainwright, Duchi, Liang, Ghahramani, Ng, Blei, Bach, Bengio, Xing, Taskar, Seeger, Chandrasekaran, ...)? What has statistics to offer to the field of machine learning? Do you think that people in the field the machine learning field are ignorant to the value of statistics? What are current modelling challenges in the field of machine learning?</p>

<p>Feel free to answer to whichever questions you like.
Thanks</p>
</div>
</li>
<li><div class="md"><p>When do you think your textbook on graphical models is going to come out? It has been several years since I used preprint portions in a machine learning class and I don&#39;t think the final book has been published yet.</p>
</div>
</li>
<li><div class="md"><p>Do you think the Google cars will really be able to drive as well as humans in ten years?  Do you think human level artificial intelligence will exist within a few decades?  Do most of the top machine learning researchers think it&#39;s likely human level AI will exist within a few decades?</p>
</div>
</li>
<li><div class="md"><p>What do you see as a final frontier of ML ? Like, what are the hardest problems, the impossible ones? </p>
</div>
</li>
<li><div class="md"><p>You have an amazing list of students. 
<a href="http://www.cs.berkeley.edu/%7Ejordan/">http://www.cs.berkeley.edu/~jordan/</a>
Nature or Nurture?</p>
</div>
</li>
<li><div class="md"><p>Can you give pointer on some recent advances in statistical mechanics of learning (after cavity method) ?
Thanks</p>
</div>
</li>
<li><div class="md"><p>Is Machine Learning fundamentally different than biologic/human learning? Or can advances in ML help understand biological cognition and vice versa? </p>
</div>
</li>
<li><div class="md"><p>What do you think is the biggest unsolved problem in machine learning besides the speed vs accuracy tradeoff that you usually talk about?</p>
</div>
</li>
<li><div class="md"><p>Thank you for doing the AMA Dr. Jordan. What can beginning data scientists do to get involved with the industry? The most compelling topics seemed to be out of reach of the mass of people interested, or it seems that the research is being conducted behind closed doors.</p>
</div>
</li>
<li><div class="md"><p>Who do you think the most exciting young (i.e., entering a tenure track position this year or next) machine learning researchers are?</p>
</div>
</li>
<li><div class="md"><p>What&#39;s your favorite stimulant or nootropic?</p>
</div>
</li>
<li><div class="md"><p>How similar is Ayasdi&#39;s topological data analysis to t-Distributed Stochastic Neighbor Embedding?</p>
</div>
<ul>
<li><div class="md"><p>Not very close, I think.  My impression is that it is a derivative of <a href="http://isomap.stanford.edu/">Isomap</a>.</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>Berkeley grad student here: What is your favorite result in number theory?</p>
</div>
</li>
<li><div class="md"><p>I read briefly the NAS report on Frontiers in Massive Data Analysis (2013) that you co-authored and I was surprised at the fact that when I searched for the keyword genomics (<a href="http://www.nap.edu/booksearch.php?booksearch=1&amp;term=genomics&amp;record_id=18374">http://www.nap.edu/booksearch.php?booksearch=1&amp;term=genomics&amp;record_id=18374</a>) , that field was really not used as a primary example (compared to, say, astronomy - see PanSTAARS example: <a href="http://www.nap.edu/openbook.php?record_id=18374&amp;page=130">http://www.nap.edu/openbook.php?record_id=18374&amp;page=130</a>) of one of the future driver for improvement in massive data science. In light of the fact that we currently have sequencers that allow us to not use combinatorial alignment algorithms (long read technology) i.e. greedy and similar algorithms are now OK. Do you think there is a collective blind spot there ?</p>
</div>
</li>
<li><div class="md"><p>What do you think of direct search methods like the Covariance Matrix Adaptation - Evolutionary Strategy?</p>
</div>
</li>
<li><div class="md"><p>1) What do you think are the very important intuition/insight behind the machine learning techniques, which would make one&#39;s learning much faster and easier after knowing them?</p>

<p>2) Do you think there will be a unified, ultimate theory/framework/structure for all these various machine learning techniques? Perhaps deep neural network a good candidate?</p>
</div>
</li>
<li><div class="md"><p>As interesting as it is, deep learning right now is mostly empirical with very little theory behind most techniques work.  How do you go about finding edge cases that fail and figuring out what made them fail?</p>
</div>
</li>
<li><div class="md"><p>What are your views on technological singularity? Given the current state of progress in ML and AI, we should be having intelligent machines soon in the future, do you think this will pose a threat to human race? What will humans do once we have machines which could do almost everything for us? </p>
</div>
</li>
<li><div class="md"><p>Hey Dr. Jordan, long time, first time.  I implemented your consensus clustering algorithm presented in &quot;Solving Consensus and Semi-supervised Clustering Problems Using Nonnegative Matrix Factorization,&quot; but the results fucking sucked.  Any thoughts on the robustness of consensus clustering and domains that it tends to perform well in?  Thanks.  </p>
</div>
</li>
<li><div class="md"><p>Thanks for doing this AMA. </p>

<p>If you had the ability to efficiently train a classification model with a non-convex objective function, which one would you choose and why?</p>

<p>Go Bears!</p>
</div>
</li>
<li><div class="md"><p>What do you think are the limitations of PGM to prevent it outperform other methods for some real applications? </p>
</div>
</li>
<li><div class="md"><p>This probably going to get buried, but what the hell is with the downvoting of half of the questions? I swear more than 50% of the questions asked have a negative score, including mine. </p>

<p>Is it because you want your questions to stick out and you achieve that by downvoting everything else? It&#39;s really disappointing.. I would expect this in AskReddit, but not here..</p>
</div>
<ul>
<li><div class="md"><p>Personally I downvoted questions that I felt were lazy and could have been answered by googling or by thinking about them for five minutes.</p>

<p>I am really happy this AMA happened, grateful for the answers (look how detailed and ... long they are) and I think wasting Professor Jordans time like that is extremely rude, to say the least.</p>

<p>As to your question, good luck with your future PhD.</p>
</div>
<ul>
<li><div class="md"><p>The heading AMA is ask me ANYTHING.  If it was not ask me <em>anything</em> it should be labelled as such. This is a very public forum. I would argue the obtuse down voting is rude.  Given his ability it excel many would be interested in his views on a wide range of concepts and this would be one of the very few times they get the opportunity to get such a direct response from a notable person.  </p>

<p>There are people from wide ranging backgrounds on here and to me it seems in appropriate to discourage asking questions.  I am certainly interested in a number of questions that are being down voted.  My personal experience in the past is that such people can provide a significant degree of enlightenment on what experts of the field may call simple questions.  The enlightenment comes from their ability to highlight critical and subtle concepts that are often miss-communicated or understood by a large number of people in the field. I found this to be the case time and time again from renowned experts, where there detail attentative descriptions has opened my eyes on concepts I had in the past dismissed as trivially understood to find later that i did not understand them.</p>

<p>In terms of googling questions?  This is a discussion forum, part of the reason we are here is because of the human factor. That element which enables us to isolate things much faster and intuitively than the poorly filtered or biased responses google often leads us too.</p>
</div>
</li>
</ul>
</li>
</ul>
</li>
<li><div class="md"><p>Where did you go?</p>
</div>
<ul>
<li><div class="md"><p>He&#39;ll be answering questions at 10 AM PST on September 10. Not sure why this post didn&#39;t mention that, but the previous one did: <a href="http://www.reddit.com/r/MachineLearning/comments/2ep8p7/machine_learning_pioneer_michael_i_jordan_will_be/">http://www.reddit.com/r/MachineLearning/comments/2ep8p7/machine_learning_pioneer_michael_i_jordan_will_be/</a>.</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>(1) Why are neural networks (e.g. feedforward neural networks, &quot;deep learning&quot;) a popular topic in machine learning at the moment?</p>

<p>(2) What are the limitations of neural networks as a technique for pattern recognition? Which methods could conceivably outperform them, and in which domains?</p>

<p>(3) Which (especially under-appreciated) topics in machine learning do you think will become popular five years from now?</p>

<p>Thanks!</p>
</div>
</li>
<li><div class="md"><p>Do you think a program can be written to one day have common sense?</p>

<p>We have all of these programs making decisions for us, and none of them has any common sense. So all decisions we make using a computer are made without common sense. No wonder we have a lot of problems.</p>
</div>
</li>
<li><div class="md"><p>Considering the huge advancement deep learning has made over the years since 2005-2006, are conventional machine learning algorithms going to extinct or there can be some course correction regarding the traditional machine learning algorithms to make them relevant? </p>
</div>
<ul>
<li><div class="md"><p>I&#39;ll answer your first question: no.</p>
</div>
</li>
</ul>
</li>
<li><div class="md"><p>Would you recommend to an aspiring machine learning computer scientist a PhD in machine learning or cognitive science?</p>
</div>
</li>
</ul>
</body>
</html>
